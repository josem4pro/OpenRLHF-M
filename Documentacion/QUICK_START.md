# ğŸš€ QUICK START - Modelo DPO Entrenado

## âš¡ EjecuciÃ³n RÃ¡pida (1 Comando)

```bash
cd /home/jose/Repositorios/OpenRLHF-M
source .venv-cpu-rlhf/bin/activate
python3 dpo_training_cpu.py
```

**Resultado**: Modelo entrenado en `dpo_output/final_model/` en ~5 minutos

---

## ğŸ”„ Re-entrenar con Diferentes ParÃ¡metros

### MÃ¡s Ã‰pocas (Mejor Calidad)
```python
# Editar dpo_training_cpu.py lÃ­nea 184
num_train_epochs=5,  # En vez de 3
```

### Dataset MÃ¡s Grande
```python
# Agregar mÃ¡s pares en lÃ­nea 66-75
preference_data = [
    # ... pares existentes ...
    {
        "prompt": "Nueva pregunta",
        "chosen": "Respuesta preferida",
        "rejected": "Respuesta rechazada"
    },
]
```

---

## ğŸ§ª Probar el Modelo Entrenado

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Cargar modelo entrenado
model = AutoModelForCausalLM.from_pretrained(
    "./dpo_output/final_model",
    torch_dtype=torch.float32,
    device_map=None
)

tokenizer = AutoTokenizer.from_pretrained("./dpo_output/final_model")

# Generar respuesta
prompt = "What is the capital of France?"
inputs = tokenizer(prompt, return_tensors="pt")

outputs = model.generate(
    **inputs,
    max_new_tokens=200,
    do_sample=False
)

response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

---

## ğŸ“Š Ver Resultados

### Reporte JSON
```bash
cat dpo_output/training_report.json | jq
```

### Log Completo
```bash
less dpo_training.log
```

### MÃ©tricas de Entrenamiento
```bash
grep "loss" dpo_training.log | tail -10
```

---

## ğŸ”§ Troubleshooting

### Error: "Out of Memory"
**SoluciÃ³n**: Reducir batch size
```python
# LÃ­nea 185
per_device_train_batch_size=1,  # En vez de 2
```

### Error: "CUDA not available"
**SoluciÃ³n**: Ya configurado para CPU - verificar:
```python
# LÃ­neas 191-194
fp16=False,
bf16=False,
use_cpu=True,
no_cuda=True,
```

### Error: "Model not found"
**SoluciÃ³n**: Descargar manualmente
```bash
# Configurar token (si no estÃ¡ en ~/.env)
export HF_TOKEN="your_huggingface_token_here"

huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct \
  --token $HF_TOKEN
```

---

## ğŸ“ Estructura de Archivos

```
OpenRLHF-M/
â”œâ”€â”€ dpo_training_cpu.py         # â† Script principal
â”œâ”€â”€ dpo_training.log            # â† Log de ejecuciÃ³n
â”œâ”€â”€ dpo_output/
â”‚   â”œâ”€â”€ final_model/            # â† Modelo entrenado (1.9GB)
â”‚   â””â”€â”€ training_report.json    # â† MÃ©tricas completas
â”œâ”€â”€ âœ…-MISION-COMPLETA-100%.md  # â† DocumentaciÃ³n exhaustiva
â””â”€â”€ QUICK_START.md              # â† Esta guÃ­a
```

---

## ğŸ¯ Comandos Ãštiles

### Ver progreso en tiempo real
```bash
tail -f dpo_training.log
```

### Verificar espacio en disco
```bash
du -sh dpo_output/
```

### Limpiar outputs anteriores
```bash
rm -rf dpo_output/
```

### Backup del modelo entrenado
```bash
tar -czf dpo_model_$(date +%Y%m%d).tar.gz dpo_output/final_model/
```

---

**Â¿Necesitas mÃ¡s ayuda?** Ver: [âœ…-MISION-COMPLETA-100%.md](âœ…-MISION-COMPLETA-100%.md)
